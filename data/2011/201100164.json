{"abstracts-retrieval-response": {"item": {"ait:process-info": {"ait:status": {"@state": "update", "@type": "core", "@stage": "S300"}, "ait:date-delivered": {"@day": "07", "@year": "2021", "@timestamp": "2021-10-07T10:17:32.000032-04:00", "@month": "10"}, "ait:date-sort": {"@day": "01", "@year": "2011", "@month": "12"}}, "bibrecord": {"head": {"author-group": [{"affiliation": {"country": "Thailand", "@afid": "60028190", "@country": "tha", "organization": [{"$": "Department of Computer Engineering"}, {"$": "Faculty of Engineering"}, {"$": "Chulalongkorn University"}], "affiliation-id": {"@afid": "60028190", "@dptid": "111225259"}, "@dptid": "111225259"}, "author": [{"ce:given-name": "Thavesak", "preferred-name": {"ce:given-name": "Thavesak", "ce:initials": "T.", "ce:surname": "Chuensaichol", "ce:indexed-name": "Chuensaichol T."}, "@seq": "1", "ce:initials": "T.", "@_fa": "true", "ce:surname": "Chuensaichol", "@auid": "54894434600", "ce:indexed-name": "Chuensaichol T."}, {"ce:given-name": "Pizzanu", "preferred-name": {"ce:given-name": "Pizzanu", "ce:initials": "P.", "ce:surname": "Kanongchaiyos", "ce:indexed-name": "Kanongchaiyos P."}, "@seq": "2", "ce:initials": "P.", "@_fa": "true", "ce:surname": "Kanongchaiyos", "@auid": "13908179500", "ce:indexed-name": "Kanongchaiyos P."}]}, {"affiliation": {"country": "Thailand", "@afid": "60011525", "@country": "tha", "organization": [{"$": "Human Language Technology Lab."}, {"$": "NECTEC"}], "affiliation-id": {"@afid": "60011525", "@dptid": "105453365"}, "@dptid": "105453365"}, "author": [{"ce:given-name": "Wutiwiwatchai", "preferred-name": {"ce:given-name": "Wutiwiwatchai", "ce:initials": "W.", "ce:surname": "Chai", "ce:indexed-name": "Chai W."}, "@seq": "3", "ce:initials": "W.", "@_fa": "true", "ce:surname": "Chai", "@auid": "6506972635", "ce:indexed-name": "Chai W."}]}], "citation-title": "Lip synchronization from Thai speech", "abstracts": "Lip synchronization in character animation is generally done in animation films and games, consuming workload and cost during the animation development process. In this paper, we focus on reducing the cost and workload in this process, and apply this technique for use with Thai speech. The main idea is to extract and capture a viseme from the video of a human talking and the phonemic scripts inside this video. First, this approach starts with separating the human talking video into two parts that contains the speech and frame sequence, then using the speech combined with a phonemic script to extract the time-stamp of each phoneme by using force-alignment techniques; next, we create a visyllable database by mapping the start time of each selected phoneme to an image; then, we capture the interested positions from the image to make a visyllable database; after that, we generate a talking head animation video by synchronizing a time-stamp of each phoneme to concatenated visemes. The results of experimental tests are reported, indicating good accuracy of the synchronized lip movement with the speech, compared to an artist-animated talking character. \u00a9 2011 ACM.", "correspondence": {"affiliation": {"country": "Thailand", "@country": "tha", "organization": [{"$": "Department of Computer Engineering"}, {"$": "Faculty of Engineering"}, {"$": "Chulalongkorn University"}]}, "person": {"ce:initials": "T.", "ce:surname": "Chuensaichol", "ce:indexed-name": "Chuensaichol T."}}, "citation-info": {"author-keywords": {"author-keyword": [{"$": "Facial animation", "@xml:lang": "eng"}, {"$": "Lip movement", "@xml:lang": "eng"}, {"$": "Lip-sync", "@xml:lang": "eng"}, {"$": "Talking characters", "@xml:lang": "eng"}, {"$": "Thai speech", "@xml:lang": "eng"}, {"$": "Visyllable database", "@xml:lang": "eng"}]}, "citation-type": {"@code": "cp"}, "citation-language": {"@language": "English", "@xml:lang": "eng"}, "abstract-language": {"@language": "English", "@xml:lang": "eng"}}, "source": {"sourcetitle-abbrev": "Proc. VRCAI: ACM SIGGRAPH Conf. Virtual Real. Continuum Its Appl. Ind.", "@country": "usa", "issuetitle": "Proceedings of VRCAI 2011: ACM SIGGRAPH Conference on Virtual-Reality Continuum and its Applications to Industry", "volisspag": {"pagerange": {"@first": "355", "@last": "358"}}, "@type": "p", "publicationyear": {"@first": "2011"}, "isbn": {"$": "9781450310604", "@length": "13"}, "additional-srcinfo": {"conferenceinfo": {"confpublication": {"procpagerange": "var.pagings"}, "confevent": {"confname": "10th International Conference on Virtual Reality Continuum and Its Applications in Industry, VRCAI'11", "confsponsors": {"confsponsor": [{"$": "ACM Spec. Interest Group Comput. Graph."}, {"$": "Interact. Tech. (SIGGRAPH)"}, {"$": "VT Committee of the China Society of Image and Graphics"}], "@complete": "y"}, "conflocation": {"city-group": "Hong Kong", "@country": "chn"}, "confcode": "88223", "confdate": {"enddate": {"@day": "12", "@year": "2011", "@month": "12"}, "startdate": {"@day": "11", "@year": "2011", "@month": "12"}}}}}, "sourcetitle": "Proceedings of VRCAI 2011: ACM SIGGRAPH Conference on Virtual-Reality Continuum and its Applications to Industry", "@srcid": "21100197504", "publicationdate": {"year": "2011", "date-text": {"@xfab-added": "true", "$": "2011"}}}, "enhancement": {"classificationgroup": {"classifications": [{"@type": "ASJC", "classification": [{"$": "1702"}, {"$": "1706"}]}, {"@type": "CPXCLASS", "classification": [{"classification-code": "723", "classification-description": "Computer Software, Data Handling and Applications"}, {"classification-code": "751.5", "classification-description": "Speech"}, {"classification-code": "961", "classification-description": "Systems Science"}]}, {"@type": "GEOCLASS", "classification": {"classification-code": "Related Topics"}}, {"@type": "SUBJABBR", "classification": "COMP"}]}}}, "item-info": {"copyright": {"$": "Copyright 2012 Elsevier B.V., All rights reserved.", "@type": "Elsevier"}, "dbcollection": [{"$": "CPX"}, {"$": "SCOPUS"}, {"$": "Scopusbase"}], "history": {"date-created": {"@day": "06", "@year": "2012", "@month": "02"}}, "itemidlist": {"itemid": [{"$": "364172091", "@idtype": "PUI"}, {"$": "20120614739892", "@idtype": "CPX"}, {"$": "84856468061", "@idtype": "SCP"}, {"$": "84856468061", "@idtype": "SGR"}], "ce:doi": "10.1145/2087756.2087814"}}, "tail": {"bibliography": {"@refcount": "12", "reference": [{"ref-fulltext": "AGGARWAL, S., AND JINDAL, A. 2008. Comprehensive overview of various lip synchronization techniques. In Biometrics and Security Technologies, 2008. ISBAST 2008. International Symposium on, IEEE, 1-6.", "@id": "1", "ref-info": {"ref-publicationyear": {"@first": "2008"}, "ref-title": {"ref-titletext": "Comprehensive overview of various lip synchronization techniques"}, "refd-itemidlist": {"itemid": {"$": "51249110902", "@idtype": "SGR"}}, "ref-volisspag": {"pagerange": {"@first": "1", "@last": "6"}}, "ref-text": "ISBAST 2008. International Symposium on, IEEE", "ref-authors": {"author": [{"@seq": "1", "ce:initials": "S.", "@_fa": "true", "ce:surname": "Aggarwal", "ce:indexed-name": "Aggarwal S."}, {"@seq": "2", "ce:initials": "A.", "@_fa": "true", "ce:surname": "Jindal", "ce:indexed-name": "Jindal A."}]}, "ref-sourcetitle": "Biometrics and Security Technologies, 2008"}}, {"ref-fulltext": "CHAMNONGTHAI, K. 2005. Final Consonant Segmentation for Thai syllable by Using Vowel Characteristics and Wavelet Packet Transform. ECTI Transactions on Computer and Information theory 1, 1, 50-62.", "@id": "2", "ref-info": {"ref-publicationyear": {"@first": "2005"}, "ref-title": {"ref-titletext": "Final consonant segmentation for thai syllable by using vowel characteristics and wavelet packet transform"}, "refd-itemidlist": {"itemid": {"$": "84856460909", "@idtype": "SGR"}}, "ref-volisspag": {"voliss": {"@volume": "1", "@issue": "1"}, "pagerange": {"@first": "50", "@last": "62"}}, "ref-authors": {"author": [{"@seq": "1", "ce:initials": "K.", "@_fa": "true", "ce:surname": "Chamnongthai", "ce:indexed-name": "Chamnongthai K."}]}, "ref-sourcetitle": "ECTI Transactions on Computer and Information Theory"}}, {"ref-fulltext": "CHUENSAICHOL, T., KANONGCHAIYOS, P., AND WUTIWIWATCHAI, C. 2011. Thai Speech-Driven Facial Animation. In Accepted to be published in proceeding of Culture and Computing 2011, IEEE.", "@id": "3", "ref-info": {"ref-publicationyear": {"@first": "2011"}, "ref-title": {"ref-titletext": "Thai speech-driven facial animation"}, "refd-itemidlist": {"itemid": {"$": "84855810453", "@idtype": "SGR"}}, "ref-text": "Accepted to be published IEEE", "ref-authors": {"author": [{"@seq": "1", "ce:initials": "T.", "@_fa": "true", "ce:surname": "Chuensaichol", "ce:indexed-name": "Chuensaichol T."}, {"@seq": "2", "ce:initials": "P.", "@_fa": "true", "ce:surname": "Kanongchaiyos", "ce:indexed-name": "Kanongchaiyos P."}, {"@seq": "3", "ce:initials": "C.", "@_fa": "true", "ce:surname": "Wutiwiwatchai", "ce:indexed-name": "Wutiwiwatchai C."}]}, "ref-sourcetitle": "Proceeding of Culture and Computing 2011"}}, {"ref-fulltext": "GRIFFIN, P., AND NOOT, H. 1995. FERSA: Lip-synchronous animation. Image Analysis Applications and Computer Graphics, 528-529. (Pubitemid 126013394)", "@id": "4", "ref-info": {"ref-publicationyear": {"@first": "1995"}, "ref-title": {"ref-titletext": "FERSA: Lip-synchronous animation"}, "refd-itemidlist": {"itemid": {"$": "84856438290", "@idtype": "SGR"}}, "ref-volisspag": {"voliss": {"@issue": "1024"}, "pagerange": {"@first": "528", "@last": "529"}}, "ref-text": "Image Analysis Applications and Computer Graphics", "ref-authors": {"author": [{"@seq": "1", "ce:initials": "P.A.", "@_fa": "true", "ce:surname": "Griffin", "ce:indexed-name": "Griffin P.A."}, {"@seq": "2", "ce:initials": "H.", "@_fa": "true", "ce:surname": "Noot", "ce:indexed-name": "Noot H."}]}, "ref-sourcetitle": "Lecture Notes in Computer Science"}}, {"ref-fulltext": "IYENGAR, G., NOCK, H., AND NETI, C. 2003. Audio-visual synchrony for detection of monologues in video archives. In 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03)., Ieee, V- 772-5.", "@id": "5", "ref-info": {"ref-publicationyear": {"@first": "2003"}, "ref-title": {"ref-titletext": "Audio-visual synchrony for detection of monologues in video archives"}, "refd-itemidlist": {"itemid": {"$": "0141631499", "@idtype": "SGR"}}, "ref-volisspag": {"pagerange": {"@first": "772", "@last": "775"}}, "ref-text": "Proceedings. (ICASSP '03)., Ieee", "ref-authors": {"author": [{"@seq": "1", "ce:initials": "G.", "@_fa": "true", "ce:surname": "Iyengar", "ce:indexed-name": "Iyengar G."}, {"@seq": "2", "ce:initials": "H.", "@_fa": "true", "ce:surname": "Nock", "ce:indexed-name": "Nock H."}, {"@seq": "3", "ce:initials": "C.", "@_fa": "true", "ce:surname": "Neti", "ce:indexed-name": "Neti C."}]}, "ref-sourcetitle": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003"}}, {"@date-locked": "2016-11-18T00:00:00.000", "ref-fulltext": "KAKUMANU, P., GUTIERREZ-OSUNA, R., ESPOSITO, A., BRYLL, R., GOSHTASBY, A., AND GARCIA, O. 2001. Speech driven facial animation. In Proceedings of the 2001 workshop on Perceptive user interfaces, ACM, 1-5.", "@id": "6", "ref-info": {"ref-publicationyear": {"@first": "2001"}, "ref-title": {"ref-titletext": "Speech driven facial animation"}, "refd-itemidlist": {"itemid": {"$": "84982786773", "@idtype": "SGR"}}, "ref-volisspag": {"pagerange": {"@first": "1", "@last": "5"}}, "ref-text": "ACM", "ref-authors": {"author": [{"@seq": "1", "ce:initials": "P.", "@_fa": "true", "ce:surname": "Kakumanu", "ce:indexed-name": "Kakumanu P."}, {"@seq": "2", "ce:initials": "R.", "@_fa": "true", "ce:surname": "Gutierrez-Osuna", "ce:indexed-name": "Gutierrez-Osuna R."}, {"@seq": "3", "ce:initials": "A.", "@_fa": "true", "ce:surname": "Esposito", "ce:indexed-name": "Esposito A."}, {"@seq": "4", "ce:initials": "R.", "@_fa": "true", "ce:surname": "Bryll", "ce:indexed-name": "Bryll R."}, {"@seq": "5", "ce:initials": "A.", "@_fa": "true", "ce:surname": "Goshtasby", "ce:indexed-name": "Goshtasby A."}, {"@seq": "6", "ce:initials": "O.", "@_fa": "true", "ce:surname": "Garcia", "ce:indexed-name": "Garcia O."}]}, "ref-sourcetitle": "Proceedings of the 2001 Workshop on Perceptive User Interfaces"}}, {"ref-fulltext": "KSHIRSAGAR, S., AND MAGNENAT-THALMANN, N. 2003. Visyllable based speech animation. In EUROGRAPHICS 2003, Wiley Online Library, vol. 22, 631-639.", "@id": "7", "ref-info": {"ref-publicationyear": {"@first": "2003"}, "ref-title": {"ref-titletext": "Visyllable based speech animation"}, "refd-itemidlist": {"itemid": {"$": "0141504400", "@idtype": "SGR"}}, "ref-volisspag": {"voliss": {"@volume": "22"}, "pagerange": {"@first": "631", "@last": "639"}}, "ref-text": "Wiley Online Library", "ref-authors": {"author": [{"@seq": "1", "ce:initials": "S.", "@_fa": "true", "ce:surname": "Kshirsagar", "ce:indexed-name": "Kshirsagar S."}, {"@seq": "2", "ce:initials": "N.", "@_fa": "true", "ce:surname": "Magnenat-Thalmann", "ce:indexed-name": "Magnenat-Thalmann N."}]}, "ref-sourcetitle": "Eurographics 2003"}}, {"ref-fulltext": "MASSARO, D. 2003. A computer-animated tutor for spoken and written language learning. In Proceedings of the 5th international conference on Multimodal interfaces, ACM, New York, New York, USA, 172-175. (Pubitemid 40681868)", "@id": "8", "ref-info": {"ref-publicationyear": {"@first": "2003"}, "ref-title": {"ref-titletext": "A computer-animated tutor for spoken and written language learning"}, "refd-itemidlist": {"itemid": {"$": "14944380820", "@idtype": "SGR"}}, "ref-volisspag": {"pagerange": {"@first": "172", "@last": "175"}}, "ref-text": "ICMI'03: Fifth International Conference on Multimodal Interfaces", "ref-authors": {"author": [{"@seq": "1", "ce:initials": "D.W.", "@_fa": "true", "ce:surname": "Massaro", "ce:indexed-name": "Massaro D.W."}]}, "ref-sourcetitle": "ICMI'03: Fifth International Conference on Multimodal Interfaces"}}, {"ref-fulltext": "MATTHEWS, I., COOTES, T., BANGHAM, J., COX, S., AND HARVEY, R. 2002. Extraction of visual features for lipreading. Pattern Analysis and Machine Intelligence, IEEE Transactions on 24, 2, 198-213. (Pubitemid 34198209)", "@id": "9", "ref-info": {"ref-publicationyear": {"@first": "2002"}, "ref-title": {"ref-titletext": "Extraction of visual features for lipreading"}, "refd-itemidlist": {"itemid": {"$": "0036472941", "@idtype": "SGR"}}, "ref-volisspag": {"voliss": {"@volume": "24", "@issue": "2"}, "pagerange": {"@first": "198", "@last": "213"}}, "ref-text": "DOI 10.1109/34.982900", "ref-authors": {"author": [{"@seq": "1", "ce:initials": "I.", "@_fa": "true", "ce:surname": "Matthews", "ce:indexed-name": "Matthews I."}, {"@seq": "2", "ce:initials": "T.F.", "@_fa": "true", "ce:surname": "Cootes", "ce:indexed-name": "Cootes T.F."}, {"@seq": "3", "ce:initials": "J.A.", "@_fa": "true", "ce:surname": "Bangham", "ce:indexed-name": "Bangham J.A."}, {"@seq": "4", "ce:initials": "S.", "@_fa": "true", "ce:surname": "Cox", "ce:indexed-name": "Cox S."}, {"@seq": "5", "ce:initials": "R.", "@_fa": "true", "ce:surname": "Harvey", "ce:indexed-name": "Harvey R."}]}, "ref-sourcetitle": "IEEE Transactions on Pattern Analysis and Machine Intelligence"}}, {"ref-fulltext": "SUEBVISAI, S., CHAROENPORNSAWAT, P., BLACK, A., WOSZCZYNA, M., AND SCHULTZ, T. 2005. Thai automatic speech recognition. In in Proc. ICASSP, IEEE, vol. 54, DTIC Document, 857-860.", "@id": "10", "ref-info": {"ref-publicationyear": {"@first": "2005"}, "ref-title": {"ref-titletext": "Thai automatic speech recognition"}, "refd-itemidlist": {"itemid": {"$": "33646815003", "@idtype": "SGR"}}, "ref-volisspag": {"voliss": {"@volume": "54"}, "pagerange": {"@first": "857", "@last": "860"}}, "ref-text": "DTIC Document", "ref-authors": {"author": [{"@seq": "1", "ce:initials": "S.", "@_fa": "true", "ce:surname": "Suebvisai", "ce:indexed-name": "Suebvisai S."}, {"@seq": "2", "ce:initials": "P.", "@_fa": "true", "ce:surname": "Charoenpornsawat", "ce:indexed-name": "Charoenpornsawat P."}, {"@seq": "3", "ce:initials": "A.", "@_fa": "true", "ce:surname": "Black", "ce:indexed-name": "Black A."}, {"@seq": "4", "ce:initials": "M.", "@_fa": "true", "ce:surname": "Woszczyna", "ce:indexed-name": "Woszczyna M."}, {"@seq": "5", "ce:initials": "T.", "@_fa": "true", "ce:surname": "Schultz", "ce:indexed-name": "Schultz T."}]}, "ref-sourcetitle": "Proc. ICASSP, IEEE"}}, {"ref-fulltext": "WATERS, K., LEVERGOOD, T., AND LABORATORY, D. E. C. C. R. 1993. DECface: An automatic lip-synchronization algorithm for synthetic faces. Citeseer, New York, New York, USA.", "@id": "11", "ref-info": {"ref-publicationyear": {"@first": "1993"}, "refd-itemidlist": {"itemid": {"$": "0004024737", "@idtype": "SGR"}}, "ref-text": "LABORATORY D. E. C. C. R. Citeseer, New York, New York, USA", "ref-authors": {"author": [{"@seq": "1", "ce:initials": "K.", "@_fa": "true", "ce:surname": "Waters", "ce:indexed-name": "Waters K."}, {"@seq": "2", "ce:initials": "T.", "@_fa": "true", "ce:surname": "Levergood", "ce:indexed-name": "Levergood T."}]}, "ref-sourcetitle": "DECface: An Automatic Lip-synchronization Algorithm for Synthetic Faces"}}, {"ref-fulltext": "YOUNG, S., GALES, M., LIU, X. A., WOODLAND, P., HTK, T., AND VERSION, H. T. K. 2009. HTK Toolkit Book. No. July 2000. Cambridge University Engineering Department.", "@id": "12", "ref-info": {"ref-publicationyear": {"@first": "2009"}, "refd-itemidlist": {"itemid": {"$": "84856485407", "@idtype": "SGR"}}, "ref-text": "No. July 2000. Cambridge University Engineering Department", "ref-authors": {"author": [{"@seq": "1", "ce:initials": "S.", "@_fa": "true", "ce:surname": "Young", "ce:indexed-name": "Young S."}, {"@seq": "2", "ce:initials": "M.", "@_fa": "true", "ce:surname": "Gales", "ce:indexed-name": "Gales M."}, {"@seq": "3", "ce:initials": "X.A.", "@_fa": "true", "ce:surname": "Liu", "ce:indexed-name": "Liu X.A."}, {"@seq": "4", "ce:initials": "P.", "@_fa": "true", "ce:surname": "Woodland", "ce:indexed-name": "Woodland P."}, {"@seq": "5", "ce:initials": "T.", "@_fa": "true", "ce:surname": "Htk", "ce:indexed-name": "Htk T."}, {"@seq": "6", "ce:initials": "H.T.K.", "@_fa": "true", "ce:surname": "Version", "ce:indexed-name": "Version H.T.K."}]}, "ref-sourcetitle": "HTK Toolkit Book"}}]}}}}, "affiliation": [{"affiliation-city": "Bangkok", "@id": "60028190", "affilname": "Chulalongkorn University", "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60028190", "affiliation-country": "Thailand"}, {"affiliation-city": "Pathum Thani", "@id": "60011525", "affilname": "Thailand National Electronics and Computer Technology Center", "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60011525", "affiliation-country": "Thailand"}], "coredata": {"srctype": "p", "eid": "2-s2.0-84856468061", "dc:description": "Lip synchronization in character animation is generally done in animation films and games, consuming workload and cost during the animation development process. In this paper, we focus on reducing the cost and workload in this process, and apply this technique for use with Thai speech. The main idea is to extract and capture a viseme from the video of a human talking and the phonemic scripts inside this video. First, this approach starts with separating the human talking video into two parts that contains the speech and frame sequence, then using the speech combined with a phonemic script to extract the time-stamp of each phoneme by using force-alignment techniques; next, we create a visyllable database by mapping the start time of each selected phoneme to an image; then, we capture the interested positions from the image to make a visyllable database; after that, we generate a talking head animation video by synchronizing a time-stamp of each phoneme to concatenated visemes. The results of experimental tests are reported, indicating good accuracy of the synchronized lip movement with the speech, compared to an artist-animated talking character. \u00a9 2011 ACM.", "prism:coverDate": "2011-12-01", "prism:aggregationType": "Conference Proceeding", "prism:url": "https://api.elsevier.com/content/abstract/scopus_id/84856468061", "subtypeDescription": "Conference Paper", "dc:creator": {"author": [{"ce:given-name": "Thavesak", "preferred-name": {"ce:given-name": "Thavesak", "ce:initials": "T.", "ce:surname": "Chuensaichol", "ce:indexed-name": "Chuensaichol T."}, "@seq": "1", "ce:initials": "T.", "@_fa": "true", "affiliation": {"@id": "60028190", "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60028190"}, "ce:surname": "Chuensaichol", "@auid": "54894434600", "author-url": "https://api.elsevier.com/content/author/author_id/54894434600", "ce:indexed-name": "Chuensaichol T."}]}, "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/84856468061"}, {"@_fa": "true", "@rel": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84856468061&origin=inward"}, {"@_fa": "true", "@rel": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=84856468061&origin=inward"}], "prism:isbn": "9781450310604", "prism:publicationName": "Proceedings of VRCAI 2011: ACM SIGGRAPH Conference on Virtual-Reality Continuum and its Applications to Industry", "source-id": "21100197504", "citedby-count": "0", "subtype": "cp", "prism:pageRange": "355-358", "dc:title": "Lip synchronization from Thai speech", "prism:endingPage": "358", "openaccess": "0", "openaccessFlag": "false", "prism:doi": "10.1145/2087756.2087814", "prism:startingPage": "355", "dc:identifier": "SCOPUS_ID:84856468061"}, "idxterms": {"mainterm": [{"$": "Facial animation", "@weight": "a", "@candidate": "n"}, {"$": "Lip movements", "@weight": "a", "@candidate": "n"}, {"$": "Lip-sync", "@weight": "a", "@candidate": "n"}, {"$": "Talking characters", "@weight": "a", "@candidate": "n"}, {"$": "Visyllable database", "@weight": "a", "@candidate": "n"}]}, "language": {"@xml:lang": "eng"}, "authkeywords": {"author-keyword": [{"@_fa": "true", "$": "Facial animation"}, {"@_fa": "true", "$": "Lip movement"}, {"@_fa": "true", "$": "Lip-sync"}, {"@_fa": "true", "$": "Talking characters"}, {"@_fa": "true", "$": "Thai speech"}, {"@_fa": "true", "$": "Visyllable database"}]}, "subject-areas": {"subject-area": [{"@_fa": "true", "$": "Artificial Intelligence", "@code": "1702", "@abbrev": "COMP"}, {"@_fa": "true", "$": "Computer Science Applications", "@code": "1706", "@abbrev": "COMP"}]}, "authors": {"author": [{"ce:given-name": "Thavesak", "preferred-name": {"ce:given-name": "Thavesak", "ce:initials": "T.", "ce:surname": "Chuensaichol", "ce:indexed-name": "Chuensaichol T."}, "@seq": "1", "ce:initials": "T.", "@_fa": "true", "affiliation": {"@id": "60028190", "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60028190"}, "ce:surname": "Chuensaichol", "@auid": "54894434600", "author-url": "https://api.elsevier.com/content/author/author_id/54894434600", "ce:indexed-name": "Chuensaichol T."}, {"ce:given-name": "Pizzanu", "preferred-name": {"ce:given-name": "Pizzanu", "ce:initials": "P.", "ce:surname": "Kanongchaiyos", "ce:indexed-name": "Kanongchaiyos P."}, "@seq": "2", "ce:initials": "P.", "@_fa": "true", "affiliation": {"@id": "60028190", "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60028190"}, "ce:surname": "Kanongchaiyos", "@auid": "13908179500", "author-url": "https://api.elsevier.com/content/author/author_id/13908179500", "ce:indexed-name": "Kanongchaiyos P."}, {"ce:given-name": "Wutiwiwatchai", "preferred-name": {"ce:given-name": "Wutiwiwatchai", "ce:initials": "W.", "ce:surname": "Chai", "ce:indexed-name": "Chai W."}, "@seq": "3", "ce:initials": "W.", "@_fa": "true", "affiliation": {"@id": "60011525", "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60011525"}, "ce:surname": "Chai", "@auid": "6506972635", "author-url": "https://api.elsevier.com/content/author/author_id/6506972635", "ce:indexed-name": "Chai W."}]}}}