{"abstracts-retrieval-response": {"item": {"ait:process-info": {"ait:status": {"@state": "update", "@type": "core", "@stage": "S300"}, "ait:date-delivered": {"@day": "18", "@year": "2019", "@timestamp": "2019-11-18T09:04:50.000050-05:00", "@month": "11"}, "ait:date-sort": {"@day": "01", "@year": "2011", "@month": "12"}}, "bibrecord": {"head": {"author-group": [{"affiliation": {"city-group": "Bangkok", "country": "Thailand", "@afid": "60028190", "@country": "tha", "organization": [{"$": "Department of Computer Engineering"}, {"$": "Chulalongkorn University"}], "affiliation-id": {"@afid": "60028190", "@dptid": "111225259"}, "@dptid": "111225259"}, "author": [{"ce:given-name": "Thavesak", "preferred-name": {"ce:given-name": "Thavesak", "ce:initials": "T.", "ce:surname": "Chuensaichol", "ce:indexed-name": "Chuensaichol T."}, "@seq": "1", "ce:initials": "T.", "@_fa": "true", "ce:surname": "Chuensaichol", "@auid": "54894434600", "ce:indexed-name": "Chuensaichol T."}, {"ce:given-name": "Pizzanu", "preferred-name": {"ce:given-name": "Pizzanu", "ce:initials": "P.", "ce:surname": "Kanongchaiyos", "ce:indexed-name": "Kanongchaiyos P."}, "@seq": "2", "ce:initials": "P.", "@_fa": "true", "ce:surname": "Kanongchaiyos", "@auid": "13908179500", "ce:indexed-name": "Kanongchaiyos P."}]}, {"affiliation": {"city-group": "Pathumthani", "country": "Thailand", "@afid": "60011525", "@country": "tha", "organization": [{"$": "Human Language Technology Laboratory"}, {"$": "NECTEC"}], "affiliation-id": {"@afid": "60011525", "@dptid": "105453365"}, "@dptid": "105453365"}, "author": [{"ce:given-name": "Chai", "preferred-name": {"ce:given-name": "Chai", "ce:initials": "C.", "ce:surname": "Wutiwiwatchai", "ce:indexed-name": "Wutiwiwatchai C."}, "@seq": "3", "ce:initials": "C.", "@_fa": "true", "ce:surname": "Wutiwiwatchai", "@auid": "6506972635", "ce:indexed-name": "Wutiwiwatchai C."}]}], "citation-title": "Thai speech-driven facial animation", "abstracts": "We consider the problem of making lip movement for an animated talking character, which consumes workload and cost during the animation development process. The main idea is to extract and capture a vise me from the video of a human talking and the phonemic scripts inside this video. After that, we generate a talking head animation video by synchronizing a time-stamped of each phoneme to concatenated visemes. The results of experimental tests are reported, indicating good accuracy. \u00a9 2011 IEEE.", "correspondence": {"affiliation": {"city-group": "Bangkok", "country": "Thailand", "@country": "tha", "organization": [{"$": "Department of Computer Engineering"}, {"$": "Chulalongkorn University"}]}, "person": {"ce:initials": "T.", "ce:surname": "Chuensaichol", "ce:indexed-name": "Chuensaichol T."}}, "citation-info": {"author-keywords": {"author-keyword": [{"$": "Facial animation", "@xml:lang": "eng"}, {"$": "Force-alignment", "@xml:lang": "eng"}, {"$": "Lip-sync", "@xml:lang": "eng"}, {"$": "Speech-driven animation", "@xml:lang": "eng"}, {"$": "Thai speech", "@xml:lang": "eng"}, {"$": "Visyllable database", "@xml:lang": "eng"}]}, "citation-type": {"@code": "cp"}, "citation-language": {"@language": "English", "@xml:lang": "eng"}, "abstract-language": {"@language": "English", "@xml:lang": "eng"}}, "source": {"sourcetitle-abbrev": "Proc. - Int. Conf. Cult. Comput., Culture Computing", "@country": "usa", "issuetitle": "Proceedings - 2011 2nd International Conference on Culture and Computing, Culture and Computing 2011", "volisspag": {"pagerange": {"@first": "121", "@last": "122"}}, "@type": "p", "publicationyear": {"@first": "2011"}, "isbn": {"$": "9780769545462", "@length": "13"}, "additional-srcinfo": {"conferenceinfo": {"confpublication": {"procpagerange": "var.pagings"}, "confevent": {"confname": "2011 2nd International Conference on Culture and Computing, Culture and Computing 2011", "confcatnumber": "P4546", "conflocation": {"city-group": "Kyoto", "@country": "jpn"}, "confcode": "88071", "confdate": {"enddate": {"@day": "22", "@year": "2011", "@month": "10"}, "startdate": {"@day": "20", "@year": "2011", "@month": "10"}}}}}, "sourcetitle": "Proceedings - 2011 2nd International Conference on Culture and Computing, Culture and Computing 2011", "article-number": "6103221", "@srcid": "21100197109", "publicationdate": {"year": "2011", "date-text": {"@xfab-added": "true", "$": "2011"}}}, "enhancement": {"classificationgroup": {"classifications": [{"@type": "ASJC", "classification": [{"$": "1703"}, {"$": "2605"}]}, {"@type": "CPXCLASS", "classification": {"classification-code": "723.5", "classification-description": "Computer Applications"}}, {"@type": "GEOCLASS", "classification": {"classification-code": "Related Topics"}}, {"@type": "SUBJABBR", "classification": [{"$": "COMP"}, {"$": "MATH"}]}]}}}, "item-info": {"copyright": {"$": "Copyright 2012 Elsevier B.V., All rights reserved.", "@type": "Elsevier"}, "dbcollection": [{"$": "CPX"}, {"$": "SCOPUS"}, {"$": "Scopusbase"}], "history": {"date-created": {"@day": "18", "@year": "2012", "@month": "01"}}, "itemidlist": {"itemid": [{"$": "364082136", "@idtype": "PUI"}, {"$": "20120314691645", "@idtype": "CPX"}, {"$": "84855810453", "@idtype": "SCP"}, {"$": "84855810453", "@idtype": "SGR"}], "ce:doi": "10.1109/Culture-Computing.2011.30"}}, "tail": {"bibliography": {"@refcount": "3", "reference": [{"ref-fulltext": "K. Waters, T. Levergood, and D. E. C. C. R. Laboratory, DECface: An automatic lip-synchronization algorithm for synthetic faces. New York, New York, USA: Citeseer, 1993. [Online]. Available: http://citeseerx.ist.psu.edu/viewdoc/ download?doi= 10.1.1.38.389\\&rep=rep1\\&type=pdf", "@id": "1", "ref-info": {"ref-publicationyear": {"@first": "1993"}, "ref-website": {"ce:e-address": {"$": "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.38.389\\&rep= rep1\\&type=pdf", "@type": "url"}}, "refd-itemidlist": {"itemid": {"$": "0004024737", "@idtype": "SGR"}}, "ref-text": "D. E. C. C. R. Laboratory, New York, New York, USA: Citeseer, [Online]. Available", "ref-authors": {"author": [{"@seq": "1", "ce:initials": "K.", "@_fa": "true", "ce:surname": "Waters", "ce:indexed-name": "Waters K."}, {"@seq": "2", "ce:initials": "T.", "@_fa": "true", "ce:surname": "Levergood", "ce:indexed-name": "Levergood T."}]}, "ref-sourcetitle": "DECface: An Automatic Lip-synchronization Algorithm for Synthetic Faces"}}, {"@date-locked": "2016-11-18T00:00:00.000", "ref-fulltext": "P. Kakumanu, R. Gutierrez-Osuna, A. Esposito, R. Bryll, A. Goshtasby, and O. Garcia, \"Speech driven facial animation,\"in Proceedings of the 2001 workshop on Perceptive user interfaces. ACM, 2001, pp. 1-5. [Online]. Available: http://portal.acm.org/citation.cfm?id=971488", "@id": "2", "ref-info": {"ref-website": {"ce:e-address": {"$": "http://portal.acm.org/citation.cfm?id=971488", "@type": "url"}}, "ref-title": {"ref-titletext": "Speech driven facial animation"}, "refd-itemidlist": {"itemid": {"$": "84982786773", "@idtype": "SGR"}}, "ref-volisspag": {"pagerange": {"@first": "1", "@last": "5"}}, "ref-text": "[Online]. Available", "ref-authors": {"author": [{"@seq": "1", "ce:initials": "P.", "@_fa": "true", "ce:surname": "Kakumanu", "ce:indexed-name": "Kakumanu P."}, {"@seq": "2", "ce:initials": "R.", "@_fa": "true", "ce:surname": "Gutierrez-Osuna", "ce:indexed-name": "Gutierrez-Osuna R."}, {"@seq": "3", "ce:initials": "A.", "@_fa": "true", "ce:surname": "Esposito", "ce:indexed-name": "Esposito A."}, {"@seq": "4", "ce:initials": "R.", "@_fa": "true", "ce:surname": "Bryll", "ce:indexed-name": "Bryll R."}, {"@seq": "5", "ce:initials": "A.", "@_fa": "true", "ce:surname": "Goshtasby", "ce:indexed-name": "Goshtasby A."}, {"@seq": "6", "ce:initials": "O.", "@_fa": "true", "ce:surname": "Garcia", "ce:indexed-name": "Garcia O."}]}, "ref-sourcetitle": "Proceedings of the 2001 Workshop on Perceptive User Interfaces. ACM, 2001"}}, {"ref-fulltext": "T. Chuensaichol, P. Kanongchaiyos, and C. Wutiwiwatchai, \"Thai Lip-Sync : Mapping Lip Movement to Thai Speech,\"Wisawakammasat Journal, pp. 33s42, 2011.", "@id": "3", "ref-info": {"ref-publicationyear": {"@first": "2011"}, "ref-title": {"ref-titletext": "Thai Lip-Sync: Mapping Lip Movement to Thai Speech"}, "refd-itemidlist": {"itemid": {"$": "84855802943", "@idtype": "SGR"}}, "ref-volisspag": {"pagerange": {"@first": "33", "@last": "42"}}, "ref-authors": {"author": [{"@seq": "1", "ce:initials": "T.", "@_fa": "true", "ce:surname": "Chuensaichol", "ce:indexed-name": "Chuensaichol T."}, {"@seq": "2", "ce:initials": "P.", "@_fa": "true", "ce:surname": "Kanongchaiyos", "ce:indexed-name": "Kanongchaiyos P."}, {"@seq": "3", "ce:initials": "C.", "@_fa": "true", "ce:surname": "Wutiwiwatchai", "ce:indexed-name": "Wutiwiwatchai C."}]}, "ref-sourcetitle": "Wisawakammasat Journal"}}]}}}}, "affiliation": [{"affiliation-city": "Bangkok", "@id": "60028190", "affilname": "Chulalongkorn University", "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60028190", "affiliation-country": "Thailand"}, {"affiliation-city": "Pathum Thani", "@id": "60011525", "affilname": "Thailand National Electronics and Computer Technology Center", "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60011525", "affiliation-country": "Thailand"}], "coredata": {"srctype": "p", "eid": "2-s2.0-84855810453", "dc:description": "We consider the problem of making lip movement for an animated talking character, which consumes workload and cost during the animation development process. The main idea is to extract and capture a vise me from the video of a human talking and the phonemic scripts inside this video. After that, we generate a talking head animation video by synchronizing a time-stamped of each phoneme to concatenated visemes. The results of experimental tests are reported, indicating good accuracy. \u00a9 2011 IEEE.", "prism:coverDate": "2011-12-01", "prism:aggregationType": "Conference Proceeding", "prism:url": "https://api.elsevier.com/content/abstract/scopus_id/84855810453", "subtypeDescription": "Conference Paper", "dc:creator": {"author": [{"ce:given-name": "Thavesak", "preferred-name": {"ce:given-name": "Thavesak", "ce:initials": "T.", "ce:surname": "Chuensaichol", "ce:indexed-name": "Chuensaichol T."}, "@seq": "1", "ce:initials": "T.", "@_fa": "true", "affiliation": {"@id": "60028190", "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60028190"}, "ce:surname": "Chuensaichol", "@auid": "54894434600", "author-url": "https://api.elsevier.com/content/author/author_id/54894434600", "ce:indexed-name": "Chuensaichol T."}]}, "link": [{"@_fa": "true", "@rel": "self", "@href": "https://api.elsevier.com/content/abstract/scopus_id/84855810453"}, {"@_fa": "true", "@rel": "scopus", "@href": "https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=84855810453&origin=inward"}, {"@_fa": "true", "@rel": "scopus-citedby", "@href": "https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=84855810453&origin=inward"}], "prism:isbn": "9780769545462", "prism:publicationName": "Proceedings - 2011 2nd International Conference on Culture and Computing, Culture and Computing 2011", "source-id": "21100197109", "citedby-count": "1", "subtype": "cp", "prism:pageRange": "121-122", "dc:title": "Thai speech-driven facial animation", "prism:endingPage": "122", "openaccess": "0", "openaccessFlag": "false", "prism:doi": "10.1109/Culture-Computing.2011.30", "prism:startingPage": "121", "article-number": "6103221", "dc:identifier": "SCOPUS_ID:84855810453"}, "idxterms": {"mainterm": [{"$": "Development process", "@weight": "a", "@candidate": "n"}, {"$": "Experimental test", "@weight": "a", "@candidate": "n"}, {"$": "Facial animation", "@weight": "a", "@candidate": "n"}, {"$": "Force-alignment", "@weight": "a", "@candidate": "n"}, {"$": "Lip movements", "@weight": "a", "@candidate": "n"}, {"$": "Lip-sync", "@weight": "a", "@candidate": "n"}, {"$": "Talking heads", "@weight": "a", "@candidate": "n"}, {"$": "Visemes", "@weight": "a", "@candidate": "n"}, {"$": "Visyllable database", "@weight": "a", "@candidate": "n"}]}, "language": {"@xml:lang": "eng"}, "authkeywords": {"author-keyword": [{"@_fa": "true", "$": "Facial animation"}, {"@_fa": "true", "$": "Force-alignment"}, {"@_fa": "true", "$": "Lip-sync"}, {"@_fa": "true", "$": "Speech-driven animation"}, {"@_fa": "true", "$": "Thai speech"}, {"@_fa": "true", "$": "Visyllable database"}]}, "subject-areas": {"subject-area": [{"@_fa": "true", "$": "Computational Theory and Mathematics", "@code": "1703", "@abbrev": "COMP"}, {"@_fa": "true", "$": "Computational Mathematics", "@code": "2605", "@abbrev": "MATH"}]}, "authors": {"author": [{"ce:given-name": "Thavesak", "preferred-name": {"ce:given-name": "Thavesak", "ce:initials": "T.", "ce:surname": "Chuensaichol", "ce:indexed-name": "Chuensaichol T."}, "@seq": "1", "ce:initials": "T.", "@_fa": "true", "affiliation": {"@id": "60028190", "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60028190"}, "ce:surname": "Chuensaichol", "@auid": "54894434600", "author-url": "https://api.elsevier.com/content/author/author_id/54894434600", "ce:indexed-name": "Chuensaichol T."}, {"ce:given-name": "Pizzanu", "preferred-name": {"ce:given-name": "Pizzanu", "ce:initials": "P.", "ce:surname": "Kanongchaiyos", "ce:indexed-name": "Kanongchaiyos P."}, "@seq": "2", "ce:initials": "P.", "@_fa": "true", "affiliation": {"@id": "60028190", "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60028190"}, "ce:surname": "Kanongchaiyos", "@auid": "13908179500", "author-url": "https://api.elsevier.com/content/author/author_id/13908179500", "ce:indexed-name": "Kanongchaiyos P."}, {"ce:given-name": "Chai", "preferred-name": {"ce:given-name": "Chai", "ce:initials": "C.", "ce:surname": "Wutiwiwatchai", "ce:indexed-name": "Wutiwiwatchai C."}, "@seq": "3", "ce:initials": "C.", "@_fa": "true", "affiliation": {"@id": "60011525", "@href": "https://api.elsevier.com/content/affiliation/affiliation_id/60011525"}, "ce:surname": "Wutiwiwatchai", "@auid": "6506972635", "author-url": "https://api.elsevier.com/content/author/author_id/6506972635", "ce:indexed-name": "Wutiwiwatchai C."}]}}}